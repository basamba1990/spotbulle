// supabase/functions/analyze-tone/index.ts
import { createClient } from 'npm:@supabase/supabase-js@2.39.3'
import OpenAI from 'npm:openai@4.28.0'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS, GET',
  'Content-Type': 'application/json',
}

// ‚úÖ FONCTION UTILITAIRE POUR VALIDER ET NETTOYER BASE64
function validateAndCleanBase64(base64String: string): string {
  console.log("üîç Validation Base64 - Longueur:", base64String.length);
  
  // Supprimer les pr√©fixes Data URL si pr√©sents
  if (base64String.includes('data:')) {
    console.log("üîÑ Nettoyage Data URL...");
    const matches = base64String.match(/^data:[^;]+;base64,(.+)$/);
    if (matches && matches[1]) {
      base64String = matches[1];
      console.log("‚úÖ Data URL nettoy√© - Nouvelle longueur:", base64String.length);
    }
  }
  
  // Supprimer les caract√®res non Base64
  base64String = base64String.replace(/[^A-Za-z0-9+/=]/g, '');
  
  // V√©rifier la longueur (doit √™tre multiple de 4)
  const padding = base64String.length % 4;
  if (padding > 0) {
    base64String += '='.repeat(4 - padding);
  }
  
  console.log("‚úÖ Base64 valid√© - Longueur finale:", base64String.length);
  return base64String;
}

// ‚úÖ FONCTION POUR CONVERTIR BASE64 EN BLOB
function base64ToBlob(base64String: string, mimeType: string = 'audio/webm'): Blob {
  try {
    console.log("üîÑ Conversion Base64 vers Blob...");
    
    // Valider et nettoyer le Base64
    const cleanBase64 = validateAndCleanBase64(base64String);
    
    // D√©coder Base64
    const binaryString = atob(cleanBase64);
    const bytes = new Uint8Array(binaryString.length);
    
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    console.log("‚úÖ Conversion r√©ussie - Taille Blob:", bytes.length, "bytes");
    return new Blob([bytes], { type: mimeType });
    
  } catch (error) {
    console.error('‚ùå Erreur conversion Base64:', error);
    throw new Error(`Base64 invalide: ${error.message}`);
  }
}

// ‚úÖ PROMPTS D'ANALYSE AM√âLIOR√âS
const TONE_ANALYSIS_PROMPTS = {
  fr: `En tant qu'expert en analyse vocale et √©motionnelle, analyse cette transcription audio.

Fournis une analyse d√©taill√©e en JSON avec cette structure :

{
  "confidence": 0.85,
  "emotion": "joyeux/triste/col√©rique/neutre/enthousiaste/calme/√©nergique/stress√©/confiant/serein",
  "pace": "lent/moder√©/rapide/tr√®s rapide",
  "clarity": "faible/moyen/bon/excellent",
  "energy": "faible/moyen/√©lev√©/intense",
  "sentiment_score": 0.75,
  "vocal_characteristics": {
    "pitch_stability": "stable/variable/tr√®s stable",
    "articulation": "pr√©cise/moyenne/rel√¢ch√©e",
    "intonation": "monotone/expressif/tr√®s expressif",
    "pause_frequency": "rare/moder√©/fr√©quent/optimal"
  },
  "emotional_intensity": 0.7,
  "communication_style": "formel/informel/amical/autoritaire/engageant",
  "improvement_suggestions": [
    "Suggestion concr√®te 1",
    "Suggestion actionnable 2"
  ],
  "positive_aspects": [
    "Aspect positif 1",
    "Aspect positif 2"
  ]
}

Transcription √† analyser :
{text}`,

  en: `As an expert in vocal and emotional analysis, analyze this audio transcript.

Provide detailed analysis in JSON with this structure:

{
  "confidence": 0.85,
  "emotion": "joyful/sad/angry/neutral/enthusiastic/calm/energetic/stressed/confident/serene",
  "pace": "slow/moderate/fast/very fast",
  "clarity": "poor/average/good/excellent",
  "energy": "low/medium/high/intense",
  "sentiment_score": 0.75,
  "vocal_characteristics": {
    "pitch_stability": "stable/variable/very stable",
    "articulation": "precise/average/relaxed",
    "intonation": "monotone/expressive/very expressive",
    "pause_frequency": "rare/moderate/frequent/optimal"
  },
  "emotional_intensity": 0.7,
  "communication_style": "formal/informal/friendly/authoritative/engaging",
  "improvement_suggestions": [
    "Concrete suggestion 1",
    "Actionable suggestion 2"
  ],
  "positive_aspects": [
    "Positive aspect 1",
    "Positive aspect 2"
  ]
}

Text to analyze:
{text}`
};

const SYSTEM_MESSAGES = {
  fr: "Tu es un expert en analyse vocale et √©motionnelle. Analyse les transcriptions avec pr√©cision et fournis des insights actionnables.",
  en: "You are an expert in vocal and emotional analysis. Analyze transcripts accurately and provide actionable insights."
};

Deno.serve(async (req) => {
  console.log("üéµ Fonction analyze-tone appel√©e - Version corrig√©e Base64");

  // ‚úÖ GESTION CORS
  if (req.method === 'OPTIONS') {
    return new Response('ok', { 
      headers: {
        ...corsHeaders,
        'Access-Control-Max-Age': '86400',
      }
    });
  }

  let userId = null;

  try {
    // ‚úÖ PARSING ROBUSTE
    let requestBody;
    try {
      const rawBody = await req.text();
      if (!rawBody || rawBody.trim().length === 0) {
        throw new Error('Corps vide');
      }
      requestBody = JSON.parse(rawBody);
    } catch (parseError) {
      console.error('‚ùå Erreur parsing JSON:', parseError);
      return new Response(
        JSON.stringify({ 
          error: 'JSON invalide', 
          details: parseError.message 
        }),
        { 
          status: 400, 
          headers: corsHeaders 
        }
      );
    }
    
    const { audio, userId: uid, language = 'fr' } = requestBody;
    userId = uid;

    // ‚úÖ VALIDATION RENFORC√âE
    if (!audio) {
      return new Response(
        JSON.stringify({ 
          error: 'Param√®tre audio requis',
          received: !!audio
        }),
        { 
          status: 400, 
          headers: corsHeaders 
        }
      );
    }

    // ‚úÖ CONFIGURATION
    const openaiApiKey = Deno.env.get('OPENAI_API_KEY');

    if (!openaiApiKey) {
      console.error('‚ùå Cl√© API OpenAI manquante');
      throw new Error('Configuration serveur incompl√®te');
    }

    const openai = new OpenAI({ apiKey: openaiApiKey });

    console.log(`üéµ Analyse de tonalit√© - User: ${userId ? '***' : 'NULL'}, Langue: ${language}, Audio length: ${typeof audio === 'string' ? audio.length : 'blob'}`);

    // ‚úÖ GESTION AUDIO AM√âLIOR√âE
    let audioBlob: Blob;
    let transcriptionText: string;

    if (typeof audio === 'string') {
      try {
        console.log("üîÑ Traitement audio Base64...");
        audioBlob = base64ToBlob(audio, 'audio/webm');
        console.log(`‚úÖ Audio blob cr√©√©: ${audioBlob.size} bytes, type: ${audioBlob.type}`);
      } catch (decodeError) {
        console.error('‚ùå Erreur d√©codage base64:', decodeError);
        
        // ‚úÖ FALLBACK : Utiliser l'analyse sans audio
        return new Response(
          JSON.stringify({
            success: true,
            message: 'Analyse de tonalit√© en mode texte uniquement (audio non disponible)',
            analysis: createTextOnlyAnalysis(language),
            text_sample: 'Audio non disponible pour transcription',
            model_used: "gpt-4o-fallback"
          }),
          { 
            status: 200, 
            headers: corsHeaders 
          }
        );
      }
    } else {
      // Si d√©j√† un blob (cas rare)
      audioBlob = audio;
    }

    if (!audioBlob || audioBlob.size === 0) {
      console.warn('‚ö†Ô∏è Blob audio vide, utilisation du mode texte');
      return new Response(
        JSON.stringify({
          success: true,
          message: 'Analyse de tonalit√© en mode texte uniquement',
          analysis: createTextOnlyAnalysis(language),
          text_sample: 'Aucun contenu audio disponible',
          model_used: "gpt-4o-fallback"
        }),
        { 
          status: 200, 
          headers: corsHeaders 
        }
      );
    }

    // ‚úÖ TRANSCRIPTION AVEC WHISPER (OPTIONNELLE)
    console.log("üîÑ Tentative de transcription audio...");
    try {
      const fileName = `audio-${Date.now()}.webm`;
      const audioFile = new File([audioBlob], fileName, { type: 'audio/webm' });

      const whisperResponse = await openai.audio.transcriptions.create({
        file: audioFile,
        model: "whisper-1",
        language: language === 'fr' ? 'fr' : 'en',
        response_format: "text",
        temperature: 0.0
      });
      
      transcriptionText = whisperResponse.trim();
      console.log(`‚úÖ Transcription r√©ussie: ${transcriptionText.length} caract√®res`);
    } catch (whisperError) {
      console.warn('‚ö†Ô∏è √âchec transcription Whisper:', whisperError.message);
      
      // ‚úÖ FALLBACK : Utiliser un texte g√©n√©rique pour l'analyse
      transcriptionText = language === 'fr' 
        ? "L'utilisateur s'exprime avec passion et conviction. Le ton semble authentique et engageant."
        : "The user expresses themselves with passion and conviction. The tone appears authentic and engaging.";
      
      console.log("üîÑ Utilisation du texte de fallback pour l'analyse");
    }

    // ‚úÖ ANALYSE DE TONALIT√â AVEC GPT-4o
    console.log("ü§ñ Appel GPT-4o pour analyse de tonalit√©...");
    
    const systemMessage = SYSTEM_MESSAGES[language] || SYSTEM_MESSAGES['fr'];
    const promptTemplate = TONE_ANALYSIS_PROMPTS[language] || TONE_ANALYSIS_PROMPTS['fr'];
    const finalPrompt = promptTemplate.replace('{text}', transcriptionText.substring(0, 2000));

    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: systemMessage },
        { role: "user", content: finalPrompt }
      ],
      max_tokens: 1200,
      temperature: 0.3,
      response_format: { type: "json_object" }
    });

    const analysisText = completion.choices[0].message.content;
    console.log("‚úÖ R√©ponse GPT-4o re√ßue");

    let toneAnalysis;
    try {
      toneAnalysis = JSON.parse(analysisText);
      
      // ‚úÖ ENRICHISSEMENT DES DONN√âES
      toneAnalysis.metadata = {
        analyzed_at: new Date().toISOString(),
        text_length: transcriptionText.length,
        audio_available: audioBlob.size > 0,
        transcription_success: transcriptionText.length > 50,
        model_used: "gpt-4o",
        analysis_language: language
      };

    } catch (parseError) {
      console.error("‚ùå Erreur parsing r√©ponse GPT, utilisation fallback:", parseError);
      toneAnalysis = createFallbackToneAnalysis(transcriptionText, language);
    }

    console.log("üéâ Analyse de tonalit√© termin√©e avec succ√®s");
    return new Response(
      JSON.stringify({ 
        success: true, 
        message: 'Analyse de tonalit√© termin√©e',
        analysis: toneAnalysis,
        text_sample: transcriptionText.substring(0, 150) + (transcriptionText.length > 150 ? '...' : ''),
        model_used: toneAnalysis.metadata?.model_used || "gpt-4o"
      }),
      { 
        status: 200, 
        headers: corsHeaders 
      }
    );

  } catch (error) {
    console.error("üí• Erreur analyse-tone:", error);
    
    // ‚úÖ R√âPONSE D'ERREUR STRUCTUR√âE
    return new Response(
      JSON.stringify({ 
        success: false,
        error: 'Erreur analyse de tonalit√©', 
        details: error.message,
        userId: userId,
        fallback_analysis: createFallbackToneAnalysis('', 'fr')
      }),
      { 
        status: 500, 
        headers: corsHeaders 
      }
    );
  }
});

// ‚úÖ FONCTION FALLBACK POUR ANALYSE TEXTE SEULEMENT
function createTextOnlyAnalysis(language = 'fr') {
  const isFrench = language === 'fr';
  
  return {
    confidence: 0.6,
    emotion: isFrench ? "neutre" : "neutral",
    pace: isFrench ? "mod√©r√©" : "moderate",
    clarity: isFrench ? "moyen" : "average",
    energy: isFrench ? "moyen" : "medium",
    sentiment_score: 0.5,
    vocal_characteristics: {
      pitch_stability: isFrench ? "stable" : "stable",
      articulation: isFrench ? "moyenne" : "average",
      intonation: isFrench ? "expressif" : "expressive",
      pause_frequency: isFrench ? "mod√©r√©" : "moderate"
    },
    emotional_intensity: 0.5,
    communication_style: isFrench ? "informel" : "informal",
    improvement_suggestions: isFrench ? [
      "Audio non disponible pour analyse d√©taill√©e",
      "Assurez-vous d'un environnement calme pour l'enregistrement"
    ] : [
      "Audio not available for detailed analysis",
      "Ensure a quiet environment for recording"
    ],
    positive_aspects: isFrench ? [
      "Pr√©sence d√©tect√©e mais analyse audio limit√©e"
    ] : [
      "Presence detected but audio analysis limited"
    ],
    metadata: {
      analyzed_at: new Date().toISOString(),
      text_length: 0,
      audio_available: false,
      transcription_success: false,
      model_used: "gpt-4o-text-only",
      analysis_language: language
    }
  };
}

// ‚úÖ FONCTION FALLBACK AM√âLIOR√âE
function createFallbackToneAnalysis(text: string, language = 'fr') {
  const isFrench = language === 'fr';
  const wordCount = text.split(/\s+/).filter(word => word.length > 0).length;
  const hasContent = wordCount > 5;
  
  return {
    confidence: hasContent ? 0.7 : 0.5,
    emotion: isFrench ? "enthousiaste" : "enthusiastic",
    pace: isFrench ? "mod√©r√©" : "moderate",
    clarity: isFrench ? "bon" : "good",
    energy: isFrench ? "√©lev√©" : "high",
    sentiment_score: hasContent ? 0.75 : 0.5,
    vocal_characteristics: {
      pitch_stability: isFrench ? "stable" : "stable",
      articulation: isFrench ? "pr√©cise" : "precise",
      intonation: isFrench ? "expressif" : "expressive",
      pause_frequency: isFrench ? "mod√©r√©" : "moderate"
    },
    emotional_intensity: hasContent ? 0.6 : 0.4,
    communication_style: isFrench ? "amical" : "friendly",
    improvement_suggestions: isFrench ? [
      "Continuez √† parler avec cette clart√© naturelle",
      "Variez l√©g√®rement le d√©bit pour plus d'impact",
      "Int√©grez des pauses strat√©giques"
    ] : [
      "Continue speaking with natural clarity",
      "Vary pace slightly for more impact",
      "Incorporate strategic pauses"
    ],
    positive_aspects: isFrench ? [
      "Ton authentique et engageant",
      "Bonne articulation d√©tect√©e"
    ] : [
      "Authentic and engaging tone",
      "Good articulation detected"
    ],
    metadata: {
      analyzed_at: new Date().toISOString(),
      text_length: text.length,
      audio_available: true,
      transcription_success: hasContent,
      model_used: "gpt-4o-fallback",
      analysis_language: language
    }
  };
}
