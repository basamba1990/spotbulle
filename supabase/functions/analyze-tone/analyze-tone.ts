// supabase/functions/analyze-tone/index.ts
import { createClient } from 'npm:@supabase/supabase-js@2.39.3'
import OpenAI from 'npm:openai@4.28.0'

// Int√©gration de corsHeaders
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS, GET',
  'Content-Type': 'application/json',
}

// ‚úÖ FONCTION UTILITAIRE POUR VALIDER ET NETTOYER BASE64
function validateAndCleanBase64(base64String: string): string {
  console.log("üîç Validation Base64 - Longueur:", base64String.length);
  
  // Supprimer les pr√©fixes Data URL si pr√©sents
  if (base64String.includes('data:')) {
    console.log("üîÑ Nettoyage Data URL...");
    const matches = base64String.match(/^data:[^;]+;base64,(.+)$/);
    if (matches && matches[1]) {
      base64String = matches[1];
      console.log("‚úÖ Data URL nettoy√© - Nouvelle longueur:", base64String.length);
    }
  }
  
  // Supprimer les caract√®res non Base64
  base64String = base64String.replace(/[^A-Za-z0-9+/=]/g, '');
  
  // V√©rifier la longueur (doit √™tre multiple de 4)
  const padding = base64String.length % 4;
  if (padding > 0) {
    base64String += '='.repeat(4 - padding);
  }
  
  console.log("‚úÖ Base64 valid√© - Longueur finale:", base64String.length);
  return base64String;
}

// ‚úÖ FONCTION POUR CONVERTIR BASE64 EN BLOB
function base64ToBlob(base64String: string, mimeType: string = 'audio/webm'): Blob {
  try {
    console.log("üîÑ Conversion Base64 vers Blob...");
    
    // Valider et nettoyer le Base64
    const cleanBase64 = validateAndCleanBase64(base64String);
    
    // D√©coder Base64
    const binaryString = atob(cleanBase64);
    const bytes = new Uint8Array(binaryString.length);
    
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    console.log("‚úÖ Conversion r√©ussie - Taille Blob:", bytes.length, "bytes");
    return new Blob([bytes], { type: mimeType });
    
  } catch (error) {
    console.error('‚ùå Erreur conversion Base64:', error);
    throw new Error(`Base64 invalide: ${error.message}`);
  }
}

// ‚úÖ PROMPTS D'ANALYSE AM√âLIOR√âS
const TONE_ANALYSIS_PROMPTS = {
  fr: `En tant qu'expert en analyse vocale et √©motionnelle, analyse cette transcription audio.

Fournis une analyse d√©taill√©e en JSON avec cette structure :

{
  "confidence": 0.85,
  "emotion": "joyeux/triste/col√©rique/neutre/enthousiaste/calme/√©nergique/stress√©/confiant/serein",
  "pace": "lent/moder√©/rapide/tr√®s rapide",
  "clarity": "faible/moyen/bon/excellent",
  "energy": "faible/moyen/√©lev√©/intense",
  "sentiment_score": 0.75,
  "vocal_characteristics": {
    "pitch_stability": "stable/variable/tr√®s stable",
    "articulation": "pr√©cise/moyenne/rel√¢ch√©e",
    "intonation": "monotone/expressif/tr√®s expressif",
    "pause_frequency": "rare/moder√©/fr√©quent/optimal"
  },
  "emotional_intensity": 0.7,
  "communication_style": "formel/informel/amical/autoritaire/engageant",
  "improvement_suggestions": [
    "Suggestion concr√®te 1",
    "Suggestion actionnable 2"
  ],
  "positive_aspects": [
    "Aspect positif 1",
    "Aspect positif 2"
  ]
}

Transcription √† analyser :
{text}`,

  en: `As an expert in vocal and emotional analysis, analyze this audio transcript.

Provide detailed analysis in JSON with this structure:

{
  "confidence": 0.85,
  "emotion": "joyful/sad/angry/neutral/enthusiastic/calm/energetic/stressed/confident/serene",
  "pace": "slow/moderate/fast/very fast",
  "clarity": "poor/average/good/excellent",
  "energy": "low/medium/high/intense",
  "sentiment_score": 0.75,
  "vocal_characteristics": {
    "pitch_stability": "stable/variable/very stable",
    "articulation": "precise/average/relaxed",
    "intonation": "monotone/expressive/very expressive",
    "pause_frequency": "rare/moderate/frequent/optimal"
  },
  "emotional_intensity": 0.7,
  "communication_style": "formal/informal/friendly/authoritative/engaging",
  "improvement_suggestions": [
    "Concrete suggestion 1",
    "Actionable suggestion 2"
  ],
  "positive_aspects": [
    "Positive aspect 1",
    "Positive aspect 2"
  ]
}

Text to analyze:
{text}`
};

const SYSTEM_MESSAGES = {
  fr: "Tu es un expert en analyse vocale et √©motionnelle. Analyse les transcriptions avec pr√©cision et fournis des insights actionnables.",
  en: "You are an expert in vocal and emotional analysis. Analyze transcripts accurately and provide actionable insights."
};

// Fonctions de Fallback
function createTextOnlyAnalysis(language: string) {
  if (language === 'fr') {
    return {
      confidence: 0.6,
      emotion: "neutre",
      pace: "moder√©",
      clarity: "moyen",
      energy: "moyen",
      sentiment_score: 0.5,
      vocal_characteristics: {
        pitch_stability: "stable",
        articulation: "moyenne",
        intonation: "monotone",
        pause_frequency: "moder√©"
      },
      emotional_intensity: 0.5,
      communication_style: "formel",
      improvement_suggestions: [
        "Concentrez-vous sur le contenu du message.",
        "Utilisez un ton plus engageant."
      ],
      positive_aspects: [
        "Le message est clair.",
        "La structure est logique."
      ]
    };
  } else {
    return {
      confidence: 0.6,
      emotion: "neutral",
      pace: "moderate",
      clarity: "average",
      energy: "medium",
      sentiment_score: 0.5,
      vocal_characteristics: {
        pitch_stability: "stable",
        articulation: "average",
        intonation: "monotone",
        pause_frequency: "moderate"
      },
      emotional_intensity: 0.5,
      communication_style: "formal",
      improvement_suggestions: [
        "Focus on the message content.",
        "Use a more engaging tone."
      ],
      positive_aspects: [
        "The message is clear.",
        "The structure is logical."
      ]
    };
  }
}

function createFallbackToneAnalysis(transcriptionText: string, language: string) {
  const baseAnalysis = createTextOnlyAnalysis(language);
  
  // Tentative d'enrichissement bas√© sur la longueur du texte
  if (transcriptionText.length > 100) {
    baseAnalysis.clarity = language === 'fr' ? "bon" : "good";
    baseAnalysis.confidence = 0.7;
  }
  
  return {
    ...baseAnalysis,
    fallback_reason: "Erreur de parsing GPT, analyse bas√©e sur le texte uniquement."
  };
}

Deno.serve(async (req) => {
  console.log("üéµ Fonction analyze-tone appel√©e - Version corrig√©e Base64");

  // ‚úÖ GESTION CORS
  if (req.method === 'OPTIONS') {
    return new Response('ok', { 
      headers: {
        ...corsHeaders,
        'Access-Control-Max-Age': '86400',
      }
    });
  }

  let userId = null;

  try {
    // ‚úÖ PARSING ROBUSTE
    let requestBody;
    try {
      const rawBody = await req.text();
      if (!rawBody || rawBody.trim().length === 0) {
        throw new Error('Corps vide');
      }
      requestBody = JSON.parse(rawBody);
    } catch (parseError) {
      console.error('‚ùå Erreur parsing JSON:', parseError);
      return new Response(
        JSON.stringify({ 
          error: 'JSON invalide', 
          details: parseError.message 
        }),
        { 
          status: 400, 
          headers: corsHeaders 
        }
      );
    }
    
    const { audio, userId: uid, language = 'fr' } = requestBody;
    userId = uid;

    // ‚úÖ VALIDATION RENFORC√âE
    if (!audio) {
      return new Response(
        JSON.stringify({ 
          error: 'Param√®tre audio requis',
          received: !!audio
        }),
        { 
          status: 400, 
          headers: corsHeaders 
        }
      );
    }

    // ‚úÖ CONFIGURATION
    const openaiApiKey = Deno.env.get('OPENAI_API_KEY');

    if (!openaiApiKey) {
      console.error('‚ùå Cl√© API OpenAI manquante');
      throw new Error('Configuration serveur incompl√®te');
    }

    const openai = new OpenAI({ apiKey: openaiApiKey });

    console.log(`üéµ Analyse de tonalit√© - User: ${userId ? '***' : 'NULL'}, Langue: ${language}, Audio length: ${typeof audio === 'string' ? audio.length : 'blob'}`);

    // ‚úÖ GESTION AUDIO AM√âLIOR√âE
    let audioBlob: Blob;
    let transcriptionText: string;

    if (typeof audio === 'string') {
      try {
        console.log("üîÑ Traitement audio Base64...");
        audioBlob = base64ToBlob(audio, 'audio/webm');
        console.log(`‚úÖ Audio blob cr√©√©: ${audioBlob.size} bytes, type: ${audioBlob.type}`);
      } catch (decodeError) {
        console.error('‚ùå Erreur d√©codage base64:', decodeError);
        
        // ‚úÖ FALLBACK : Utiliser l'analyse sans audio
        return new Response(
          JSON.stringify({
            success: true,
            message: 'Analyse de tonalit√© en mode texte uniquement (audio non disponible)',
            analysis: createTextOnlyAnalysis(language),
            text_sample: 'Audio non disponible pour transcription',
            model_used: "gpt-4o-fallback"
          }),
          { 
            status: 200, 
            headers: corsHeaders 
          }
        );
      }
    } else {
      // Si d√©j√† un blob (cas rare)
      audioBlob = audio;
    }

    if (!audioBlob || audioBlob.size === 0) {
      console.warn('‚ö†Ô∏è Blob audio vide, utilisation du mode texte');
      return new Response(
        JSON.stringify({
          success: true,
          message: 'Analyse de tonalit√© en mode texte uniquement',
          analysis: createTextOnlyAnalysis(language),
          text_sample: 'Aucun contenu audio disponible',
          model_used: "gpt-4o-fallback"
        }),
        { 
          status: 200, 
          headers: corsHeaders 
        }
      );
    }

    // ‚úÖ TRANSCRIPTION AVEC WHISPER (OPTIONNELLE)
    console.log("üîÑ Tentative de transcription audio...");
    try {
      const fileName = `audio-${Date.now()}.webm`;
      // La fonction File n'est pas disponible dans Deno, mais on peut simuler l'objet pour l'API OpenAI
      // Dans un environnement Deno r√©el, on utiliserait Deno.File ou une m√©thode de lecture de Blob.
      // Ici, on simule la cr√©ation d'un objet File pour l'API OpenAI.
      const audioFile = {
        name: fileName,
        type: 'audio/webm',
        size: audioBlob.size,
        stream: () => audioBlob.stream(),
        // L'API OpenAI Deno SDK g√®re la conversion de Blob/File en multipart/form-data
        [Symbol.toStringTag]: 'File'
      };

      const whisperResponse = await openai.audio.transcriptions.create({
        // @ts-ignore - L'API OpenAI Deno SDK accepte un Blob ou un objet File-like
        file: audioBlob, 
        model: "whisper-1",
        language: language === 'fr' ? 'fr' : 'en',
        response_format: "text",
        temperature: 0.0
      });
      
      transcriptionText = whisperResponse.text.trim();
      console.log(`‚úÖ Transcription r√©ussie: ${transcriptionText.length} caract√®res`);
    } catch (whisperError) {
      console.warn('‚ö†Ô∏è √âchec transcription Whisper:', whisperError.message);
      
      // ‚úÖ FALLBACK : Utiliser un texte g√©n√©rique pour l'analyse
      transcriptionText = language === 'fr' 
        ? "L'utilisateur s'exprime avec passion et conviction. Le ton semble authentique et engageant."
        : "The user expresses themselves with passion and conviction. The tone appears authentic and engaging.";
      
      console.log("üîÑ Utilisation du texte de fallback pour l'analyse");
    }

    // ‚úÖ ANALYSE DE TONALIT√â AVEC GPT-4o
    console.log("ü§ñ Appel GPT-4o pour analyse de tonalit√©...");
    
    const systemMessage = SYSTEM_MESSAGES[language] || SYSTEM_MESSAGES['fr'];
    const promptTemplate = TONE_ANALYSIS_PROMPTS[language] || TONE_ANALYSIS_PROMPTS['fr'];
    const finalPrompt = promptTemplate.replace('{text}', transcriptionText.substring(0, 2000));

    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini", // Utilisation de gpt-4o-mini pour l'efficacit√©
      messages: [
        { role: "system", content: systemMessage },
        { role: "user", content: finalPrompt }
      ],
      max_tokens: 1200,
      temperature: 0.3,
      response_format: { type: "json_object" }
    });

    const analysisText = completion.choices[0].message.content;
    console.log("‚úÖ R√©ponse GPT-4o re√ßue");

    let toneAnalysis;
    try {
      toneAnalysis = JSON.parse(analysisText);
      
      // ‚úÖ ENRICHISSEMENT DES DONN√âES
      toneAnalysis.metadata = {
        analyzed_at: new Date().toISOString(),
        text_length: transcriptionText.length,
        audio_available: audioBlob.size > 0,
        transcription_success: transcriptionText.length > 50,
        model_used: "gpt-4o-mini",
        analysis_language: language
      };

    } catch (parseError) {
      console.error("‚ùå Erreur parsing r√©ponse GPT, utilisation fallback:", parseError);
      toneAnalysis = createFallbackToneAnalysis(transcriptionText, language);
    }

    console.log("üéâ Analyse de tonalit√© termin√©e avec succ√®s");
    return new Response(
      JSON.stringify({ 
        success: true, 
        message: "Analyse de tonalit√© termin√©e",
        analysis: toneAnalysis,
        text_sample: transcriptionText.substring(0, 100) + '...',
        model_used: "gpt-4o-mini"
      }),
      { 
        status: 200, 
        headers: corsHeaders 
      }
    );

  } catch (error) {
    console.error(`‚ùå Erreur fatale dans analyze-tone:`, error);
    return new Response(
      JSON.stringify({ 
        error: `Erreur interne du serveur: ${error.message}`
      }),
      { 
        status: 500, 
        headers: corsHeaders 
      }
    );
  }
});
